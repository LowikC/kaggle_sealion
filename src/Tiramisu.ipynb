{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.core import Dropout, Activation, Reshape\n",
    "from keras.layers.convolutional import Convolution2D, Deconvolution2D, AtrousConvolution2D, UpSampling2D\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNetFCN(input_shape, batch_norm=True, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,\n",
    "                reduction=0.0, dropout_rate=0.0, weight_decay=1E-4, init_conv_filters=48, classes=1):\n",
    "    \"\"\"Instantiate the DenseNet FCN architecture.\n",
    "        Note that when using TensorFlow,\n",
    "        for best performance you should set\n",
    "        `image_dim_ordering=\"tf\"` in your Keras config\n",
    "        at ~/.keras/keras.json.\n",
    "        # Arguments\n",
    "            nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
    "            growth_rate: number of filters to add per dense block\n",
    "            nb_layers_per_block: number of layers in each dense block.\n",
    "                Can be a positive integer or a list.\n",
    "                If positive integer, a set number of layers per dense block.\n",
    "                If list, nb_layer is used as provided. Note that list size must\n",
    "                be (nb_dense_block + 1)\n",
    "            reduction: reduction factor of transition blocks.\n",
    "                Note : reduction value is inverted to compute compression.\n",
    "            dropout_rate: dropout rate\n",
    "            weight_decay: weight decay factor\n",
    "            init_conv_filters: number of layers in the initial convolution layer\n",
    "            input_shape: optional shape tuple, only to be specified\n",
    "                It should have exactly 3 inputs channels,\n",
    "                and width and height should be no smaller than 8.\n",
    "                E.g. `(200, 200, 3)` would be one valid value.\n",
    "            classes: optional number of classes to classify images\n",
    "                into, only to be specified if `include_top` is True, and\n",
    "                if no `weights` argument is specified.\n",
    "            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.\n",
    "                Note that if sigmoid is used, classes must be 1.\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "    \"\"\"\n",
    "    if type(nb_layers_per_block) is not list and nb_dense_block < 1:\n",
    "        raise ValueError('Number of dense layers per block must be greater than 1. Argument '\n",
    "                         'value was %d.' % (nb_layers_per_block))\n",
    "\n",
    "    # Determine proper input shape\n",
    "    min_size = 2 ** nb_dense_block\n",
    "\n",
    "    if input_shape is not None:\n",
    "        if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
    "                (input_shape[1] is not None and input_shape[1] < min_size)):\n",
    "            raise ValueError('Input size must be at least ' +\n",
    "                             str(min_size) + 'x' + str(min_size) + ', got '\n",
    "                                                                   '`input_shape=' + str(input_shape) + '`')\n",
    "    else:\n",
    "        input_shape = (None, None, classes)\n",
    "\n",
    "\n",
    "    img_input = Input(shape=input_shape)\n",
    "    x = __create_fcn_dense_net(classes, img_input, batch_norm, nb_dense_block,\n",
    "                               growth_rate, reduction, dropout_rate, weight_decay,\n",
    "                               nb_layers_per_block,init_conv_filters, input_shape)\n",
    "\n",
    "    # Create model.\n",
    "    model = Model(img_input, x, name='fcn-densenet')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def __conv_block(ip, nb_filter, batch_norm=True, bottleneck=False, dropout_rate=None, weight_decay=1E-4):\n",
    "    ''' Apply BatchNorm, Relu, 3x3 Conv2D, optional bottleneck block and dropout\n",
    "    Args:\n",
    "        ip: Input keras tensor\n",
    "        nb_filter: number of filters\n",
    "        bottleneck: add bottleneck block\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay factor\n",
    "    Returns: keras tensor with batch_norm, relu and convolution2d added (optional bottleneck)\n",
    "    '''\n",
    "\n",
    "    concat_axis = -1\n",
    "\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                                beta_regularizer=l2(weight_decay))(ip)\n",
    "    else:\n",
    "        x = ip\n",
    "        \n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if bottleneck:\n",
    "        inter_channel = nb_filter * 4  # Obtained from https://github.com/liuzhuang13/DenseNet/blob/master/densenet.lua\n",
    "\n",
    "        x = Convolution2D(inter_channel, 1, 1, init='he_uniform', border_mode='same', bias=False,\n",
    "                          W_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "        if dropout_rate:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "\n",
    "        if batch_norm:\n",
    "            x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                                   beta_regularizer=l2(weight_decay))(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    x = Convolution2D(nb_filter, (3, 3), padding=\"same\", use_bias=False,\n",
    "                      kernel_initializer=\"he_uniform\",\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def __transition_block(ip, nb_filter, batch_norm=True, compression=1.0, dropout_rate=None, weight_decay=1E-4):\n",
    "    ''' Apply BatchNorm, Relu 1x1, Conv2D, optional compression, dropout and Maxpooling2D\n",
    "    Args:\n",
    "        ip: keras tensor\n",
    "        nb_filter: number of filters\n",
    "        compression: calculated as 1 - reduction. Reduces the number of feature maps\n",
    "                    in the transition block.\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay factor\n",
    "    Returns: keras tensor, after applying batch_norm, relu-conv, dropout, maxpool\n",
    "    '''\n",
    "\n",
    "    concat_axis = -1\n",
    "\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization(axis=concat_axis, gamma_regularizer=l2(weight_decay),\n",
    "                               beta_regularizer=l2(weight_decay))(ip)\n",
    "    else:\n",
    "        x = ip\n",
    "        \n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(int(nb_filter * compression), (1, 1), padding=\"same\", use_bias=False,\n",
    "                      kernel_initializer=\"he_uniform\",\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def __dense_block(x, nb_layers, nb_filter, growth_rate, batch_norm=True, bottleneck=False, dropout_rate=None, weight_decay=1E-4,\n",
    "                  grow_nb_filters=True, return_concat_list=False):\n",
    "    ''' Build a dense_block where the output of each conv_block is fed to subsequent ones\n",
    "    Args:\n",
    "        x: keras tensor\n",
    "        nb_layers: the number of layers of conv_block to append to the model.\n",
    "        nb_filter: number of filters\n",
    "        growth_rate: growth rate\n",
    "        bottleneck: bottleneck block\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay factor\n",
    "        grow_nb_filters: flag to decide to allow number of filters to grow\n",
    "        return_concat_list: return the list of feature maps along with the actual output\n",
    "    Returns: keras tensor with nb_layers of conv_block appended\n",
    "    '''\n",
    "\n",
    "    concat_axis = -1\n",
    "\n",
    "    x_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = __conv_block(x, growth_rate, batch_norm, bottleneck, dropout_rate, weight_decay)\n",
    "        x_list.append(x)\n",
    "\n",
    "        x = Concatenate(axis=concat_axis)(x_list)\n",
    "\n",
    "        if grow_nb_filters:\n",
    "            nb_filter += growth_rate\n",
    "\n",
    "    if return_concat_list:\n",
    "        return x, nb_filter, x_list\n",
    "    else:\n",
    "        return x, nb_filter\n",
    "\n",
    "\n",
    "def __transition_up_block(ip):\n",
    "    ''' SubpixelConvolutional Upscaling (factor = 2)\n",
    "    Args:\n",
    "        ip: keras tensor\n",
    "    Returns: keras tensor, after applying upsampling operation.\n",
    "    '''\n",
    "    x = UpSampling2D()(ip)\n",
    "    return x\n",
    "\n",
    "\n",
    "def __create_fcn_dense_net(nb_classes, img_input, batch_norm=True, nb_dense_block=5, growth_rate=12,\n",
    "                           reduction=0.0, dropout_rate=None, weight_decay=1E-4,\n",
    "                           nb_layers_per_block=4, init_conv_filters=48,\n",
    "                           input_shape=None):\n",
    "    ''' Build the DenseNet model\n",
    "    Args:\n",
    "        nb_classes: number of classes\n",
    "        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
    "        nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
    "        growth_rate: number of filters to add per dense block\n",
    "        reduction: reduction factor of transition blocks. Note : reduction value is inverted to compute compression\n",
    "        dropout_rate: dropout rate\n",
    "        weight_decay: weight decay\n",
    "        nb_layers_per_block: number of layers in each dense block.\n",
    "            Can be a positive integer or a list.\n",
    "            If positive integer, a set number of layers per dense block.\n",
    "            If list, nb_layer is used as provided. Note that list size must\n",
    "            be (nb_dense_block + 1)\n",
    "        input_shape: Only used for shape inference in fully convolutional networks.\n",
    "    Returns: keras tensor with nb_layers of conv_block appended\n",
    "    '''\n",
    "\n",
    "    concat_axis = -1\n",
    "    rows, cols, _ = input_shape\n",
    "\n",
    "    if reduction != 0.0:\n",
    "        assert reduction <= 1.0 and reduction > 0.0, \"reduction value must lie between 0.0 and 1.0\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n",
    "        nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n",
    "\n",
    "        assert len(nb_layers) == (nb_dense_block + 1), \"If list, nb_layer is used as provided. \" \\\n",
    "                                                       \"Note that list size must be (nb_dense_block + 1)\"\n",
    "\n",
    "        bottleneck_nb_layers = nb_layers[-1]\n",
    "        rev_layers = nb_layers[::-1]\n",
    "        nb_layers.extend(rev_layers[1:])\n",
    "    else:\n",
    "        bottleneck_nb_layers = nb_layers_per_block\n",
    "        nb_layers = [nb_layers_per_block] * (2 * nb_dense_block + 1)\n",
    "\n",
    "    # compute compression factor\n",
    "    compression = 1.0 - reduction\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Convolution2D(init_conv_filters, (3, 3), padding=\"same\", use_bias=False, \n",
    "                      kernel_initializer=\"he_uniform\",\n",
    "                      kernel_regularizer=l2(weight_decay),\n",
    "                      name=\"initial_conv2D\")(img_input)\n",
    "\n",
    "    nb_filter = init_conv_filters\n",
    "\n",
    "    skip_list = []\n",
    "\n",
    "    # Add dense blocks and transition down block\n",
    "    for block_idx in range(nb_dense_block):\n",
    "        x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter, growth_rate,\n",
    "                                     batch_norm=batch_norm,\n",
    "                                     dropout_rate=dropout_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Skip connection\n",
    "        skip_list.append(x)\n",
    "\n",
    "        # add transition_block\n",
    "        x = __transition_block(x, nb_filter,\n",
    "                               batch_norm=batch_norm,\n",
    "                               compression=compression, dropout_rate=dropout_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "\n",
    "        nb_filter = int(nb_filter * compression)  # this is calculated inside transition_down_block\n",
    "\n",
    "    # The last dense_block does not have a transition_down_block\n",
    "    # return the concatenated feature maps without the concatenation of the input\n",
    "    _, nb_filter, concat_list = __dense_block(x, bottleneck_nb_layers, nb_filter, growth_rate,\n",
    "                                              batch_norm=batch_norm,\n",
    "                                              dropout_rate=dropout_rate, weight_decay=weight_decay,\n",
    "                                              return_concat_list=True)\n",
    "\n",
    "    skip_list = skip_list[::-1]  # reverse the skip list\n",
    "\n",
    "    # Add dense blocks and transition up block\n",
    "    for block_idx in range(nb_dense_block):\n",
    "        n_filters_keep = growth_rate * nb_layers[nb_dense_block + block_idx]\n",
    "\n",
    "        # upsampling block must upsample only the feature maps (concat_list[1:]),\n",
    "        # not the concatenation of the input with the feature maps (concat_list[0].\n",
    "        l = Concatenate(axis=concat_axis)(concat_list[1:])\n",
    "\n",
    "        t = __transition_up_block(l)\n",
    "\n",
    "        # concatenate the skip connection with the transition block\n",
    "        x = Concatenate(axis=concat_axis)([t, skip_list[block_idx]])\n",
    "\n",
    "        # Dont allow the feature map size to grow in upsampling dense blocks\n",
    "        _, nb_filter, concat_list = __dense_block(x, nb_layers[nb_dense_block + block_idx + 1], nb_filter=growth_rate,\n",
    "                                                  growth_rate=growth_rate, batch_norm=batch_norm,\n",
    "                                                  dropout_rate=dropout_rate,\n",
    "                                                  weight_decay=weight_decay,\n",
    "                                                  return_concat_list=True, grow_nb_filters=False)\n",
    "\n",
    "\n",
    "    x = Convolution2D(nb_classes, (1, 1), padding='same', use_bias=False,\n",
    "                      kernel_regularizer=l2(weight_decay),\n",
    "                      activation='linear')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2f801af0b8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFTdJREFUeJzt3X+sZGV9x/H3Z+beXdiFlkXoQhYsC11NobUrbiiJSrRU\nBdK60D8otFG0xNUEEk1sGsCkJSYm1oqmpi1mCdSloSAtIvyBVSRGYirID5GfAssKYddlF3Hl18L9\nMfPtH+c5d88zO7N35s7cOzPbzyu5mZlnztz5zp0zn3mec849jyICM7NSbdgFmNlocSiYWcahYGYZ\nh4KZZRwKZpZxKJhZZtFCQdJZkp6UtFXSZYv1PGY2WFqM4xQk1YGngA8A24H7gAsj4vGBP5mZDdRi\n9RROA7ZGxLaImAZuAjYu0nOZ2QBNLNLvXQM8X7m9HfjjTguvWLU8dNSRrJicJkLUVPReahSXZV9G\nSpdleyi/v/I7fZymHaw0zx01mgDMRh2AV6YOIRpi+rkdv4qIo+f7/YsVCvOStAnYBHDMmjr//t1V\n/OGyPbwZcHiteHUrNAlAM73ISRUvspY6ODPRyO6vVTo+ZZvZOGqkr7V6JQLKtknqbR9TT9+ay9Pn\nZk9jLzMEn3/h/fzvzhP42Z9/4blunnuxQmEHcHzl9nGpbU5EbAY2A/z+O5bH41Nr+PHr69gzu4J6\n+kC/MnsoAFPN4o/wWxNTANSUQkB5f6DeRRA0vMPFxkiz0nNupp5xeXu6mX98l9VmAXijUYTCtleP\nYmp2gl++9NvUtx3a9XMuVijcB6yTtJYiDC4A/qrTwgHsmV3Ji9OHM9OsM1kregDli5tOoTDZEgZl\nOJTqmn/Q0IiOnS+zkdOM2tx63oziC6283RoK5bpdfm6mZid4Y2aS5kyNWg+f9EUJhYiYlXQp8F2g\nDlwXEY91Wv43syu5e/fvMd2o02jWmGkUL356tiiv2UzDieUzANRrxR9Fc+FQXB7o4+5tDHawqLf0\nHMowKNtnm8Xn59cvr2R2pk7snWDi9e6/DBdtm0JE3AHc0ctjRPFBr5UbFMsPexor7bvdPgxahxPt\nNN1TsDHXup5HS3v5+UCBFPS6yg9tQ2NGxQuarDeoNWtzL26iXgwjZhvF8GHFZNFTKD/Yk+n+1r0V\n3WgesF9htjQ6rbMHWj9bH1OGwCH1YpvCbBSfodlGnTdnJnjt9cmegsFb3cwsMxI9BRHUFLw5O5F1\n76dm8vKmUo+hPD6hdWtsua2huozZOGu396G6nsP+w4llqQc9PVtnZqaOGiLtmOiKewpmlhmJngIU\n46Q3pieJ0FwilpflmCmmJ7PHTKv92Kod9xxsHEVo3/rf8nko1Wt5T/mQiaJbMDU9wcz0BDRAPRzL\n556CmWVGpqcAZL0E6PzNP7dMyzaFam/Aux7tYBBA664Dtaz3zSgPiS6Uxyk0GzWiIdSUewpmtnAj\n0VOIENPNOrPNGs2m5v4bsjySsdSs5QdnzO2FaD1og/3HX96mYKOsU6+4ut6W12u18rDn8oOSega1\nfUc0SkGjUSMaNWoNUKP7WtxTMLPMSPQUqlT5Qt//2733b333EGwcdFpPq3sf9lu2i8P6icpPl9xT\nMLPMaPQU0v8+VMf/UsyNneYWU/ttCnPtbZYt+b8kbRTN14+NNr2BfacO6PAPgop9RzkuoKM8GqGw\nQK1hUD3cs3WXpAcRNq7m1t0OB+u120i5b7d978/n4YOZZRbcU5B0PHA9sJoijzZHxD9LuhL4BPBi\nWvSKdG6FrsydjDW038aX1rPR77fbsc19ZuPggLsky4OU0q7HcljdSLfrLbso577py18Z6mqbZKmf\n4cMs8NmIeFDS4cADku5M9301Ir7cx+82syFZcChExE5gZ7r+qqQnKE7tbmYDNF+vd9C94oFsU5B0\nAvBO4N7UdKmkhyVdJ2nVIJ7DzJZG36Eg6TDgFuAzEfEKcDVwErCeoidxVYfHbZJ0v6T79+6Z6rcM\ns4OWKrvru73/QMvPp69QkDRJEQg3RMS3ACJiV0Q0IqIJXEMxhdx+ImJzRGyIiA0rVi3vpwwzG6AF\nh4KKUyxfCzwREV+ptB9bWew84NGFl2dmS62fvQ/vBj4CPCLpodR2BXChpPUUO0SeBT7ZV4VmtqT6\n2fvwI9ofKNjTXA9mNnjhIxrNbFAcCmaWcSiYWcahYGYZh4KZZRwKZpZxKJhZxqFgZhmHgpllHApm\nlnEomFnGoWBmGYeCmWUcCmaWcSiYWcahYGaZvqeNk/Qs8CrQAGYjYoOkI4FvAidQnH3p/IjY0+9z\nmdniG1RP4f0RsT4iNqTblwF3RcQ64K5028zGwGINHzYCW9L1LcC5i/Q8ZjZggwiFAL4n6QFJm1Lb\n6jSDFMALFPNNZjzvg9loGsRU9O+JiB2Sfge4U9LPq3dGRKjNzBQRsRnYDHDMKUcufOYKMxuovnsK\nEbEjXe4GbqWY/GVXOf9Dutzd7/OY2dLod4aolWnGaSStBD5IMfnL7cBFabGLgNv6eR4zWzr9Dh9W\nA7cWk0UxAfxnRPyPpPuAmyVdDDwHnN/n85jZEukrFCJiG/BHbdpfAs7s53eb2XD4iEYzyzgUzCzj\nUDCzjEPBzDIOBTPLOBTMLONQMLOMQ8HMMg4FM8s4FMws41Aws4xDwcwyDgUzyzgUzCzjUDCzzILP\npyDp7RRzO5ROBP4eOAL4BPBiar8iIu5YcIVmtqQWHAoR8SSwHkBSHdhBcY7GjwNfjYgvD6RCM1tS\ngxo+nAk8ExHPDej3mdmQDCoULgBurNy+VNLDkq6TtGpAz2FmS6DvUJC0DPgw8F+p6WrgJIqhxU7g\nqg6P82QwZiNoED2Fs4EHI2IXQETsiohGRDSBayjmgdhPRGyOiA0RsWHFquUDKMPMBmEQoXAhlaFD\nOQlMch7FPBBmNib6OsV7mgDmA8AnK81fkrSeYo7JZ1vuM7MR1++8D68Db2lp+0hfFZnZUPmIRjPL\nOBTMLONQMLOMQ8HMMg4FM8s4FMws41Aws4xDwcwyDgUzyzgUzCzjUDCzjEPBzDIOBTPLOBTMLONQ\nMLNMV6GQTsC6W9KjlbYjJd0p6el0uSq1S9LXJG1NJ289dbGKN7PB67an8A3grJa2y4C7ImIdcFe6\nDcU5G9eln00UJ3I1szHRVShExN3Ar1uaNwJb0vUtwLmV9uujcA9wRMt5G81shPWzTWF1ROxM118A\nVqfra4DnK8ttT21mNgYGsqExIoLiRK1d87wPZqOpn1DYVQ4L0uXu1L4DOL6y3HGpLeN5H8xGUz+h\ncDtwUbp+EXBbpf2jaS/E6cDLlWGGmY24rk7xLulG4H3AUZK2A/8AfBG4WdLFwHPA+WnxO4BzgK3A\nXopZqM1sTHQVChFxYYe7zmyzbACX9FOUmQ2Pj2g0s4xDwcwyDgUzyzgUzCzjUDCzjEPBzDIOBTPL\nOBTMLONQMLOMQ8HMMg4FM8s4FMws41Aws4xDwcwyDgUzyzgUzCwzbyh0mAjmnyT9PE32cqukI1L7\nCZLekPRQ+vn6YhZvZoPXTU/hG+w/EcydwB9ExDuAp4DLK/c9ExHr08+nBlOmmS2VeUOh3UQwEfG9\niJhNN++hOGOzmR0EBrFN4W+A71Rur5X0U0k/lPTeTg/yvA9mo6mrE7d2IulzwCxwQ2raCbw1Il6S\n9C7g25JOiYhXWh8bEZuBzQDHnHJkTxPJmNniWXBPQdLHgD8D/jqdwZmImIqIl9L1B4BngLcNoE4z\nWyILCgVJZwF/B3w4IvZW2o+WVE/XT6SYeXrbIAo1s6Ux7/Chw0QwlwPLgTslAdyT9jScAXxe0gzQ\nBD4VEa2zVZvZCJs3FDpMBHNth2VvAW7ptygzGx4f0WhmGYeCmWUcCmaWcSiYWcahYGYZh4KZZRwK\nZpZxKJhZxqFgZhmHgpllHApmlnEomFnGoWBmGYeCmWUcCmaWWei8D1dK2lGZ3+Gcyn2XS9oq6UlJ\nH1qsws1scSx03geAr1bmd7gDQNLJwAXAKekx/1aens3MxsOC5n04gI3ATekErr8AtgKn9VGfmS2x\nfrYpXJqmjbtO0qrUtgZ4vrLM9tS2H8/7YDaaFhoKVwMnAesp5nq4qtdfEBGbI2JDRGxYsWr5Assw\ns0FbUChExK6IaEREE7iGfUOEHcDxlUWPS21mNiYWOu/DsZWb5wHlnonbgQskLZe0lmLeh5/0V6KZ\nLaWFzvvwPknrgQCeBT4JEBGPSboZeJxiOrlLIqKxOKWb2WIY6LwPafkvAF/opygzGx4f0WhmGYeC\nmWUcCmaWcSiYWcahYGYZh4KZZRwKZpZxKJhZxqFgZhmHgpllHApmlnEomFnGoWBmGYeCmWUcCmaW\nWei8D9+szPnwrKSHUvsJkt6o3Pf1xSzezAZv3pOsUMz78C/A9WVDRPxleV3SVcDLleWfiYj1gyrQ\nzJZWN2deulvSCe3ukyTgfOBPBluWmQ1Lv9sU3gvsioinK21rJf1U0g8lvbfP329mS6yb4cOBXAjc\nWLm9E3hrRLwk6V3AtyWdEhGvtD5Q0iZgE8Dhx67oswwzG5QF9xQkTQB/AXyzbEvTxb2Urj8APAO8\nrd3jPRmM2WjqZ/jwp8DPI2J72SDp6HJCWUknUsz7sK2/Es1sKXWzS/JG4MfA2yVtl3RxuusC8qED\nwBnAw2kX5X8Dn4qIbienNbMRsNB5H4iIj7VpuwW4pf+yzGxYfESjmWUcCmaWcSiYWcahYGYZh4KZ\nZRwKZpZxKJhZxqFgZhmHgpllHApmlnEomFnGoWBmGYeCmWUcCmaWcSiYWaabk6wcL+kHkh6X9Jik\nT6f2IyXdKenpdLkqtUvS1yRtlfSwpFMX+0W0rzva/piNksVaL+d+p3p/bDc9hVngsxFxMnA6cImk\nk4HLgLsiYh1wV7oNcDbFadjWUZyY9ereyzKzYZk3FCJiZ0Q8mK6/CjwBrAE2AlvSYluAc9P1jcD1\nUbgHOELSsQOvfB4RavtjNkoWfb0M6LUj0tM2hTQpzDuBe4HVEbEz3fUCsDpdXwM8X3nY9tRmZmOg\n61CQdBjF+Rc/0zqPQ0QE0FMeSdok6X5J9+/dM9XLQ/2tb/+v9Lu+9/rQrkJB0iRFINwQEd9KzbvK\nYUG63J3adwDHVx5+XGrLC/W8D2YjqZu9DwKuBZ6IiK9U7roduChdvwi4rdL+0bQX4nTg5cowoyfz\nbZmNKH723XbvwcZTL9u+5lum9TPT6zaFbqaNezfwEeCRcsp54Argi8DNaR6I5ygmmgW4AzgH2Ars\nBT7eW0lmNkzdzPvwIzrv7TyzzfIBXNJrIbUDxFmZfPv3GtR2ObNxc6B1t7yv7B20Ljtvz6LHDnS/\nE8wOhMhfdDkkaH2xZfu+P1J5u/3y7XiIYaOk9QPfql17azjM92WoHncD+DBnM8uMRE8hEM25jSxF\nElY3Mu7fY1DLZQ/P1dK7MFtK/fRuu113a9VDnJWGDz2s7+4pmFlmJHoKNZpM1hocumyGZkCzWWRV\npw0qB9rw4o2NdjApe83lddh/HZ+sNwGo14pLUfQW1GMPoeSegpllRqKnAEWyLZuYpdGsoYkGAJMp\n+ZrlmKrlMY02PYrWXZtlUHbamel+hQ3TfF/kUVmm0bL9od36DzBR9hgUqBaoCTS7r8k9BTPLjERP\nQRTpdtjkNLOxL6eW12eBfYlYaqbsnE3tZTIC1Ohu363ZKGnt4TYPcHxC6/rfen92/MICPgfuKZhZ\nZiR6ClAkoxTUiLlv+5lmveOykPcQoOgllClZJm+57IEOozYbFbUO6y8w962vsi19pc+2bFsoH1uf\naNJs1oiJICa63w3hnoKZZUampwDw2vRyAqinpJusF3shJrRvayrAbLMou9PWV7Nx06knW+0ptC5T\n3i57zPX0OZlqFJ+Puf8hqhc/XdfS/aJmttTahUVN0dVweKGH8o9ET2E2arw2vZxf7j4CAuqTTaSg\n0Sgyq1ZLR3Sl5aORXm154f9jsIPYfv/zUAZCy96GWj3vUc/sWEltWtRnYPKV7nvTIxEKEhy2bIra\nRBNC1NPBS+UultbbTVqGDR4+2MGqza5JlV+SabUvvzTrc6FQtE9PBNFIX6I9fHGORCjseW0l276/\nllW/DCqHKaAiC+bGQ9Ey2FEPR2mZja2W77zW78AyNxrL05dmOuXpEW8URzM2DhHLXu3+wzISobDs\nN8Ex90yx/N6niDenIIoXELPFwUuaSGXWUzo0y+6TU8EObtFs0wtuXe+VhtkrVxQ3DzmkWGzvXpqv\nv079bScRhy7r+jkVvZyMYJFIehF4HfjVsGvpw1GMd/0w/q9h3OuHxX0NvxsRR8+30EiEAoCk+yNi\nw7DrWKhxrx/G/zWMe/0wGq/BuyTNLONQMLPMKIXC5mEX0Kdxrx/G/zWMe/0wAq9hZLYpmNloGKWe\ngpmNgKGHgqSzJD0paauky4ZdT7ckPSvpEUkPSbo/tR0p6U5JT6fLVcOus0rSdZJ2S3q00ta25jQX\n6NfS+/KwpFOHV/lcre3qv1LSjvQ+PCTpnMp9l6f6n5T0oeFUvY+k4yX9QNLjkh6T9OnUPlrvQUQM\n7QeoA88AJwLLgJ8BJw+zph5qfxY4qqXtS8Bl6fplwD8Ou86W+s4ATgUena9mivlAv0NxgOzpwL0j\nWv+VwN+2WfbktD4tB9am9aw+5PqPBU5N1w8Hnkp1jtR7MOyewmnA1ojYFhHTwE3AxiHX1I+NwJZ0\nfQtw7hBr2U9E3A38uqW5U80bgeujcA9whKRjl6bS9jrU38lG4KaImIqIX1BMeHzaohXXhYjYGREP\npuuvAk8Aaxix92DYobAGeL5ye3tqGwcBfE/SA5I2pbbVEbEzXX8BWD2c0nrSqeZxem8uTd3r6ypD\ntpGuX9IJwDuBexmx92DYoTDO3hMRpwJnA5dIOqN6ZxT9v7HatTOONQNXAycB64GdwFXDLWd+kg4D\nbgE+ExGvVO8bhfdg2KGwAzi+cvu41DbyImJHutwN3ErRNd1Vdu/S5e7hVdi1TjWPxXsTEbsiohER\nTeAa9g0RRrJ+SZMUgXBDRHwrNY/UezDsULgPWCdpraRlwAXA7UOuaV6SVko6vLwOfBB4lKL2i9Ji\nFwG3DafCnnSq+Xbgo2kL+OnAy5Uu7shoGWOfR/E+QFH/BZKWS1oLrAN+stT1VUkScC3wRER8pXLX\naL0Hw9waW9nC+hTF1uHPDbueLms+kWLL9s+Ax8q6gbcAdwFPA98Hjhx2rS1130jRxZ6hGJ9e3Klm\nii3e/5rel0eADSNa/3+k+h6m+BAdW1n+c6n+J4GzR6D+91AMDR4GHko/54zae+AjGs0sM+zhg5mN\nGIeCmWUcCmaWcSiYWcahYGYZh4KZZRwKZpZxKJhZ5v8AguBk3tfpDyoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f83073cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DenseNetFCN(input_shape=(None, None, 3), batch_norm=False, nb_dense_block=3, growth_rate=12)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "t = np.ones((1, 224, 224, 3))\n",
    "\n",
    "p = model.predict(t)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "p.shape\n",
    "\n",
    "plt.imshow(p[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "initial_conv2D (Conv2D)      (None, None, None, 48)    1296      \n",
      "_________________________________________________________________\n",
      "activation_742 (Activation)  (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_751 (Conv2D)          (None, None, None, 12)    5184      \n",
      "_________________________________________________________________\n",
      "concatenate_567 (Concatenate (None, None, None, 60)    0         \n",
      "_________________________________________________________________\n",
      "activation_743 (Activation)  (None, None, None, 60)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_752 (Conv2D)          (None, None, None, 12)    6480      \n",
      "_________________________________________________________________\n",
      "concatenate_568 (Concatenate (None, None, None, 72)    0         \n",
      "_________________________________________________________________\n",
      "activation_744 (Activation)  (None, None, None, 72)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_753 (Conv2D)          (None, None, None, 12)    7776      \n",
      "_________________________________________________________________\n",
      "concatenate_569 (Concatenate (None, None, None, 84)    0         \n",
      "_________________________________________________________________\n",
      "activation_745 (Activation)  (None, None, None, 84)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_754 (Conv2D)          (None, None, None, 12)    9072      \n",
      "_________________________________________________________________\n",
      "concatenate_570 (Concatenate (None, None, None, 96)    0         \n",
      "_________________________________________________________________\n",
      "activation_746 (Activation)  (None, None, None, 96)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_755 (Conv2D)          (None, None, None, 96)    9216      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_79 (Averag (None, None, None, 96)    0         \n",
      "_________________________________________________________________\n",
      "activation_747 (Activation)  (None, None, None, 96)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_756 (Conv2D)          (None, None, None, 12)    10368     \n",
      "_________________________________________________________________\n",
      "concatenate_571 (Concatenate (None, None, None, 108)   0         \n",
      "_________________________________________________________________\n",
      "activation_748 (Activation)  (None, None, None, 108)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_757 (Conv2D)          (None, None, None, 12)    11664     \n",
      "_________________________________________________________________\n",
      "concatenate_572 (Concatenate (None, None, None, 120)   0         \n",
      "_________________________________________________________________\n",
      "activation_749 (Activation)  (None, None, None, 120)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_758 (Conv2D)          (None, None, None, 12)    12960     \n",
      "_________________________________________________________________\n",
      "concatenate_573 (Concatenate (None, None, None, 132)   0         \n",
      "_________________________________________________________________\n",
      "activation_750 (Activation)  (None, None, None, 132)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_759 (Conv2D)          (None, None, None, 12)    14256     \n",
      "_________________________________________________________________\n",
      "concatenate_574 (Concatenate (None, None, None, 144)   0         \n",
      "_________________________________________________________________\n",
      "activation_751 (Activation)  (None, None, None, 144)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_760 (Conv2D)          (None, None, None, 144)   20736     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_80 (Averag (None, None, None, 144)   0         \n",
      "_________________________________________________________________\n",
      "activation_752 (Activation)  (None, None, None, 144)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_761 (Conv2D)          (None, None, None, 12)    15552     \n",
      "_________________________________________________________________\n",
      "concatenate_575 (Concatenate (None, None, None, 156)   0         \n",
      "_________________________________________________________________\n",
      "activation_753 (Activation)  (None, None, None, 156)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_762 (Conv2D)          (None, None, None, 12)    16848     \n",
      "_________________________________________________________________\n",
      "concatenate_576 (Concatenate (None, None, None, 168)   0         \n",
      "_________________________________________________________________\n",
      "activation_754 (Activation)  (None, None, None, 168)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_763 (Conv2D)          (None, None, None, 12)    18144     \n",
      "_________________________________________________________________\n",
      "concatenate_577 (Concatenate (None, None, None, 180)   0         \n",
      "_________________________________________________________________\n",
      "activation_755 (Activation)  (None, None, None, 180)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_764 (Conv2D)          (None, None, None, 12)    19440     \n",
      "_________________________________________________________________\n",
      "concatenate_578 (Concatenate (None, None, None, 192)   0         \n",
      "_________________________________________________________________\n",
      "activation_756 (Activation)  (None, None, None, 192)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_765 (Conv2D)          (None, None, None, 192)   36864     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_81 (Averag (None, None, None, 192)   0         \n",
      "_________________________________________________________________\n",
      "activation_757 (Activation)  (None, None, None, 192)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_766 (Conv2D)          (None, None, None, 12)    20736     \n",
      "_________________________________________________________________\n",
      "concatenate_579 (Concatenate (None, None, None, 204)   0         \n",
      "_________________________________________________________________\n",
      "activation_758 (Activation)  (None, None, None, 204)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_767 (Conv2D)          (None, None, None, 12)    22032     \n",
      "_________________________________________________________________\n",
      "concatenate_580 (Concatenate (None, None, None, 216)   0         \n",
      "_________________________________________________________________\n",
      "activation_759 (Activation)  (None, None, None, 216)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_768 (Conv2D)          (None, None, None, 12)    23328     \n",
      "_________________________________________________________________\n",
      "concatenate_581 (Concatenate (None, None, None, 228)   0         \n",
      "_________________________________________________________________\n",
      "activation_760 (Activation)  (None, None, None, 228)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_769 (Conv2D)          (None, None, None, 12)    24624     \n",
      "_________________________________________________________________\n",
      "concatenate_583 (Concatenate (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_69 (UpSampling (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "concatenate_584 (Concatenate (None, None, None, 240)   0         \n",
      "_________________________________________________________________\n",
      "activation_761 (Activation)  (None, None, None, 240)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_770 (Conv2D)          (None, None, None, 12)    25920     \n",
      "_________________________________________________________________\n",
      "concatenate_585 (Concatenate (None, None, None, 252)   0         \n",
      "_________________________________________________________________\n",
      "activation_762 (Activation)  (None, None, None, 252)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_771 (Conv2D)          (None, None, None, 12)    27216     \n",
      "_________________________________________________________________\n",
      "concatenate_586 (Concatenate (None, None, None, 264)   0         \n",
      "_________________________________________________________________\n",
      "activation_763 (Activation)  (None, None, None, 264)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_772 (Conv2D)          (None, None, None, 12)    28512     \n",
      "_________________________________________________________________\n",
      "concatenate_587 (Concatenate (None, None, None, 276)   0         \n",
      "_________________________________________________________________\n",
      "activation_764 (Activation)  (None, None, None, 276)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_773 (Conv2D)          (None, None, None, 12)    29808     \n",
      "_________________________________________________________________\n",
      "concatenate_589 (Concatenate (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_70 (UpSampling (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "concatenate_590 (Concatenate (None, None, None, 192)   0         \n",
      "_________________________________________________________________\n",
      "activation_765 (Activation)  (None, None, None, 192)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_774 (Conv2D)          (None, None, None, 12)    20736     \n",
      "_________________________________________________________________\n",
      "concatenate_591 (Concatenate (None, None, None, 204)   0         \n",
      "_________________________________________________________________\n",
      "activation_766 (Activation)  (None, None, None, 204)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_775 (Conv2D)          (None, None, None, 12)    22032     \n",
      "_________________________________________________________________\n",
      "concatenate_592 (Concatenate (None, None, None, 216)   0         \n",
      "_________________________________________________________________\n",
      "activation_767 (Activation)  (None, None, None, 216)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_776 (Conv2D)          (None, None, None, 12)    23328     \n",
      "_________________________________________________________________\n",
      "concatenate_593 (Concatenate (None, None, None, 228)   0         \n",
      "_________________________________________________________________\n",
      "activation_768 (Activation)  (None, None, None, 228)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_777 (Conv2D)          (None, None, None, 12)    24624     \n",
      "_________________________________________________________________\n",
      "concatenate_595 (Concatenate (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_71 (UpSampling (None, None, None, 48)    0         \n",
      "_________________________________________________________________\n",
      "concatenate_596 (Concatenate (None, None, None, 144)   0         \n",
      "_________________________________________________________________\n",
      "conv2d_782 (Conv2D)          (None, None, None, 1)     144       \n",
      "=================================================================\n",
      "Total params: 508,896\n",
      "Trainable params: 508,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
