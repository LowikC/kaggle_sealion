{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find a way to predict counts, assuming we have a 4x segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, AveragePooling2D, UpSampling2D, Flatten, Dense, Activation, Cropping2D, Reshape, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error, categorical_crossentropy\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Reshape, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from pkg_resources import parse_version\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorBoardCallBack(Callback):\n",
    "    \"\"\"Tensorboard basic visualizations.\n",
    "    This callback writes a log for TensorBoard, which allows\n",
    "    you to visualize dynamic graphs of your training and test\n",
    "    metrics, as well as activation histograms for the different\n",
    "    layers in your model.\n",
    "    TensorBoard is a visualization tool provided with TensorFlow.\n",
    "    If you have installed TensorFlow with pip, you should be able\n",
    "    to launch TensorBoard from the command line:\n",
    "    ```\n",
    "    tensorboard --logdir=/full_path_to_your_logs\n",
    "    ```\n",
    "    You can find more information about TensorBoard\n",
    "    [here](https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html).\n",
    "    # Arguments\n",
    "        log_dir: the path of the directory where to save the log\n",
    "            files to be parsed by Tensorboard\n",
    "        batch_freq: frequency (in batch) at which to log data\n",
    "            If set to 0, we just log at the end of an epoch,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log_dir='./logs',\n",
    "                 batch_freq=0):\n",
    "        super(TensorBoardCallBack, self).__init__()\n",
    "\n",
    "        self.log_dir = log_dir\n",
    "        self.batch_freq = batch_freq\n",
    "        self.merged = None\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "        self.last_batch = 0\n",
    "        self.batch_offset = 0\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self.sess = K.get_session()\n",
    "\n",
    "        if hasattr(tf, 'merge_all_summaries'):\n",
    "            self.merged = tf.merge_all_summaries()\n",
    "        else:\n",
    "            self.merged = tf.summary.merge_all()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Each time we go back to batch 0, we increase the batch_offset\n",
    "        if batch < self.last_batch:\n",
    "            self.batch_offset += self.last_batch + 1\n",
    "        self.last_batch = batch\n",
    "\n",
    "        batch_cross_epoch = self.batch_offset + batch\n",
    "        if batch_cross_epoch % self.batch_freq == 0:\n",
    "            logs = logs or {}\n",
    "\n",
    "            for name, value in logs.items():\n",
    "                if name in ['batch', 'size']:\n",
    "                    continue\n",
    "                summary = tf.Summary()\n",
    "                summary_value = summary.value.add()\n",
    "                summary_value.simple_value = value.item()\n",
    "                summary_value.tag = name\n",
    "                self.writer.add_summary(summary, batch_cross_epoch)\n",
    "            self.writer.flush()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(summary, epoch)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def on_train_end(self, _):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OHELayer(Layer):\n",
    "\n",
    "    def __init__(self, n_classes, **kwargs):\n",
    "        self.n_classes = n_classes\n",
    "        super(OHELayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(OHELayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        x = K.cast(x, tf.int32)\n",
    "        y = K.one_hot(x, num_classes=self.n_classes)\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.n_classes, )\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'n_classes': self.n_classes}\n",
    "        base_config = super(OHELayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SumLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(SumLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.sum(K.sum(x, axis=1), axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], ) + (input_shape[-1], )\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(SumLayer, self).get_config()\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cntnet(im_height, im_width, n_channels=1):\n",
    "    inputs = Input((im_height, im_width, n_channels))\n",
    "    x = OHELayer(6)(inputs)\n",
    "    print(x.shape)\n",
    "    x = SumLayer()(x)\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32, kernel_initializer='uniform')(x)\n",
    "    x = Dense(5, kernel_initializer='constant')(x)\n",
    "    model = Model(inputs=[inputs], outputs=[x], name=\"cnt_net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cntnet(im_height, im_width, n_channels=1):\n",
    "    inputs = Input((im_height, im_width, n_channels))\n",
    "    x = OHELayer(6)(inputs)\n",
    "    x = Reshape((256, 256, 6))(x)\n",
    "    print(x.shape)\n",
    "    x = AveragePooling2D((8, 8))(x)\n",
    "    print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(5, kernel_initializer='constant')(x)\n",
    "    model = Model(inputs=[inputs], outputs=[x], name=\"cnt_net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 256, 256, 6)\n",
      "(?, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "unet = get_cntnet( 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts output:  (1, 5)\n"
     ]
    }
   ],
   "source": [
    "t = np.ones((1, 256, 256, 1))\n",
    "counts = unet.predict(t)\n",
    "print(\"Counts output: \", counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "ohe_layer_5 (OHELayer)       (None, 256, 256, 1, 6)    0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 32, 32, 6)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 30725     \n",
      "=================================================================\n",
      "Total params: 30,725\n",
      "Trainable params: 30,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NonValidPatch(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PatchIterator(Iterator):\n",
    "    \"\"\"Iterator yielding training samples\n",
    "    :param root_dir: Directory containing training images, density map and sampling map.\n",
    "    :param image_ids: Set of image ids to use to sample patches.\n",
    "    :param n_samples_per_image: Number of patches to sample on each image.\n",
    "    :param target_size: Size of the patches sampled.\n",
    "    :param batch_size: Number of patches sampled per batch\n",
    "    :param shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "    :param seed: Random seed for data shuffling.\n",
    "    :return batch_x, batch_x. \n",
    "        batch_x is a (batch_size, target_size[0], target_size[1], 3) array\n",
    "        batch_x is a (batch_size, target_size[0], target_size[1], 1) array if output_counts is False\n",
    "        otherwise, it is a (batch_size, 5) array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_ids,\n",
    "                 class_weights,\n",
    "                 n_samples_per_image=160,\n",
    "                 target_size=(256, 256),\n",
    "                 scale = 1,\n",
    "                 crop = (0, 0),\n",
    "                 batch_size=16, shuffle=True, seed=42,\n",
    "                 debug_dir=None):\n",
    "        self.n_samples_per_block = 8\n",
    "        self.n_sealion_types = 5\n",
    "        self.image_ids = image_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.debug_dir = debug_dir\n",
    "        # Normalize to use class_weights as a probability distribution.\n",
    "        if class_weights:\n",
    "            self.class_weights = np.asarray(class_weights)/np.sum(class_weights)\n",
    "        else:\n",
    "            self.class_weights = np.ones((self.n_sealion_types+1))/(self.n_sealion_types + 1)\n",
    "        self.n_samples_per_image = n_samples_per_image\n",
    "        self.target_size = target_size\n",
    "        self.crop = crop\n",
    "        self.scale = scale\n",
    "        self.n_indices = len(self.image_ids) * self.n_samples_per_image\n",
    "        super(PatchIterator, self).__init__(self.n_indices, batch_size//self.n_samples_per_block, shuffle, seed)\n",
    "        \n",
    "    def compute_class_distribution(self, n_batches):\n",
    "        seg_freqs = defaultdict(int)\n",
    "        count_freqs = {0: defaultdict(int),\n",
    "                             1: defaultdict(int),\n",
    "                             2: defaultdict(int),\n",
    "                             3: defaultdict(int),\n",
    "                             4: defaultdict(int)}\n",
    "        for b in range(n_batches):\n",
    "            _, [by, bcounts] = self.next()\n",
    "            by = np.argmax(by, axis=-1)\n",
    "            ids, freqs = np.unique(by, return_counts=True)\n",
    "            for i in range(ids.shape[0]):\n",
    "                seg_freqs[ids[i]] += freqs[i]\n",
    "            for b in range(bcounts.shape[0]):\n",
    "                counts = bcounts[b]\n",
    "                for i in range(counts.shape[0]):\n",
    "                    count_freqs[i][counts[i]] += 1\n",
    "        \n",
    "        return seg_freqs, count_freqs\n",
    "\n",
    "    def normalize_input(self, x_bgr):\n",
    "        x = x_bgr.copy()\n",
    "        return x\n",
    "    \n",
    "    def denormalize_input(self, x_normed):\n",
    "        x = x_normed.copy()\n",
    "        return x\n",
    "\n",
    "    def normalize_output(self, y):\n",
    "        yc = y.copy()\n",
    "        for i in range(y.shape[0]):\n",
    "            yc[i] = self.quantify(y[i])\n",
    "        return to_categorical(yc, 64)\n",
    "    \n",
    "    def quantify(self, y):\n",
    "        if y == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(y//2 + 1, 63)\n",
    "        \n",
    "    def dequantify(self, y_normed):\n",
    "        if y_normed == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            cnt = (y_normed - 1) * 2\n",
    "            if cnt == 0:\n",
    "                cnt = 1\n",
    "            return cnt\n",
    "        \n",
    "    def denormalize_output(self, y_normed): \n",
    "        yc = y_normed.copy()\n",
    "        for i in range(y_normed.shape[0]):\n",
    "            yc[i] = self.dequantifiy(y_normed[i])\n",
    "        return yc\n",
    "        \n",
    "    def random_transform(self, x):\n",
    "        flips = np.random.randint(0, 2, (3,))\n",
    "        if flips[0]:\n",
    "            x = np.rot90(x)\n",
    "        if flips[1]:\n",
    "            x = np.flipud(x)\n",
    "        if flips[2]:\n",
    "            x = np.fliplr(x)\n",
    "        return x\n",
    "                     \n",
    "    def get_dots_in_patch(self, sx, sy, dots):\n",
    "        dots_in_patch = [[] for _ in dots]\n",
    "        for i, ds in enumerate(dots):\n",
    "            for (x, y) in ds:\n",
    "                if sx <= x < sx + self.target_size[1] and sy <= y < sy + self.target_size[0]:\n",
    "                    dots_in_patch[i].append((x - sx, y - sy))\n",
    "        return dots_in_patch\n",
    "    \n",
    "    def build_counts(self, dots):\n",
    "        h = (self.target_size[0] - 2 * self.crop[0])//self.scale\n",
    "        w = (self.target_size[1] - 2 * self.crop[1])//self.scale\n",
    "        counts = np.zeros((5, ), dtype=np.int32)\n",
    "        for c, ds in enumerate(dots):\n",
    "            for (x, y) in ds:\n",
    "                xr = int(round((x - self.crop[1])/self.scale))\n",
    "                yr = int(round((y - self.crop[0])/self.scale))\n",
    "\n",
    "                if 0 <= xr < w and 0 <= yr < h:\n",
    "                    counts[c] += 1\n",
    "        return counts\n",
    "          \n",
    "    def get_weights(self, dots):\n",
    "        # Set probability to 0 if some sealion type is not in the block\n",
    "        current_weigths = self.class_weights.copy()\n",
    "        for i in range(self.n_sealion_types):\n",
    "            if not dots[i]:\n",
    "                current_weigths[i] = 0\n",
    "        current_weigths /= np.sum(current_weigths)\n",
    "        return current_weigths\n",
    "    \n",
    "    def sample(self, shape, dots, image_id):\n",
    "        margin = self.crop[0] + 30\n",
    "        max_iterations = self.n_samples_per_block * 5\n",
    "        \n",
    "        smap = np.load(os.path.join(self.root_dir, \"TrainSegmentationIgnored4x\", str(image_id) + \".npz\"))[\"smap\"]                                                                          \n",
    "        n_samples = 0\n",
    "        bx = np.zeros((self.n_samples_per_block, self.target_size[0], self.target_size[1]))\n",
    "        bcounts = np.zeros((self.n_samples_per_block, 5))\n",
    "\n",
    "        current_iteration = 0\n",
    "        weights = self.get_weights(dots)\n",
    "        \n",
    "        # Samples n dots, with some probabilty to get the background only\n",
    "        while n_samples < self.n_samples_per_block and current_iteration < max_iterations:\n",
    "            current_iteration += 1\n",
    "            try:\n",
    "                # Choose an output class randomly\n",
    "                output_class = np.random.choice(self.n_sealion_types + 1, size=(1, ), p=weights)[0]\n",
    "                \n",
    "                # Background, select randomly a patch in the image (high chance to get only background)\n",
    "                if output_class == self.n_sealion_types:\n",
    "                    sx_min = 0\n",
    "                    sx_max = shape[1] - self.target_size[1]\n",
    "                    sy_min = 0\n",
    "                    sy_max = shape[0] - self.target_size[0]\n",
    "                # Choose a dot randomly in that class\n",
    "                else:  \n",
    "                    dot_index = np.random.randint(0, len(dots[output_class]))\n",
    "                    rx, ry = dots[output_class][dot_index]\n",
    "                    sx_min = min(max(0, rx - self.target_size[1] + margin), shape[1] - self.target_size[1])\n",
    "                    sx_max = min(max(0, rx - margin), shape[1] - self.target_size[1])\n",
    "                    sy_min = min(max(0, ry - self.target_size[0] + margin), shape[0] - self.target_size[1])\n",
    "                    sy_max = min(max(0, ry - margin), shape[0] - self.target_size[0]) \n",
    "                \n",
    "                    if sx_min >= sx_max:\n",
    "                        sx_min = min(max(0, rx - self.target_size[1]), shape[1] - self.target_size[1])\n",
    "                        sx_max = min(max(0, rx), shape[1] - self.target_size[1])\n",
    "                        \n",
    "                    if sy_min >= sy_max:\n",
    "                        sy_min = min(max(0, ry - self.target_size[0]), shape[0] - self.target_size[1])\n",
    "                        sy_max = min(max(0, ry), shape[0] - self.target_size[0]) \n",
    "                    \n",
    "                if sx_min >= sx_max or sy_min >= sy_max:\n",
    "                        continue\n",
    "                        \n",
    "                # Choose the top-left corner so that the dot selected is in the patch.\n",
    "                sx = np.random.randint(sx_min, sx_max)\n",
    "                sy = np.random.randint(sy_min, sy_max)\n",
    "\n",
    "                dots_in_patch = self.get_dots_in_patch(sx, sy, dots)\n",
    "                \n",
    "                smap_patch = smap[sy:sy+self.target_size[0], sx:sx+self.target_size[1]]\n",
    "                smap_patch[smap_patch==6]=5\n",
    "                counts = self.build_counts(dots_in_patch)\n",
    "                smap_patch = self.random_transform(smap_patch)\n",
    "                bx[n_samples, ...] = smap_patch\n",
    "                bcounts[n_samples, ...] = counts\n",
    "                n_samples += 1\n",
    "            except NonValidPatch:\n",
    "                continue\n",
    "                    \n",
    "        if current_iteration < max_iterations:\n",
    "            return bx, bcounts\n",
    "        else:\n",
    "            print(\"Error with image \", image_id)\n",
    "            raise Exception(\"hoho\")\n",
    "    \n",
    "        \n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "                \n",
    "        batch_x = np.zeros((current_batch_size * self.n_samples_per_block,\n",
    "                            self.target_size[0],\n",
    "                            self.target_size[1]),\n",
    "                           dtype=K.floatx())\n",
    "        batch_counts = np.zeros((current_batch_size * self.n_samples_per_block, 5), dtype=np.int32)\n",
    "        \n",
    "        # For each index, we load the data and sample randomly n_successive_samples patches\n",
    "        for i, j in enumerate(index_array):\n",
    "            index = j // self.n_samples_per_image\n",
    "            image_id = self.image_ids[index]\n",
    "            with open(os.path.join(self.root_dir, \"TrainDots\", str(image_id) + \".pkl\"), \"rb\") as pfile:\n",
    "                dots = pickle.load(pfile)\n",
    "            with open(os.path.join(self.root_dir, \"TrainShape\", str(image_id) + \".pkl\"), \"rb\") as pfile:\n",
    "                shape = pickle.load(pfile)\n",
    "            \n",
    "            # Scale dots coordinates\n",
    "            dots_resized = []\n",
    "            for ds in dots:\n",
    "                ds_resized = []\n",
    "                for (x, y) in ds:\n",
    "                    ds_resized.append((x//4, y//4))\n",
    "                dots_resized.append(ds_resized)\n",
    "                \n",
    "            shape = (shape[0]//4, shape[1]//4)\n",
    "                                      \n",
    "                \n",
    "            x, counts = self.sample(shape, dots_resized, image_id)\n",
    "            batch_x[i*self.n_samples_per_block:(i+1)*self.n_samples_per_block, ...] = x\n",
    "            batch_counts[i*self.n_samples_per_block:(i+1)*self.n_samples_per_block, ...] = counts\n",
    "        \n",
    "        b, h, w = batch_x.shape\n",
    "        return self.normalize_input(batch_x).reshape((b, h, w, 1)), batch_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/sealion/train.json\", \"r\") as jfile:\n",
    "    train_ids = json.load(jfile)\n",
    "train_ids = [int(iid) for iid in train_ids]\n",
    "\n",
    "with open(\"../data/sealion/val.json\", \"r\") as jfile:\n",
    "    val_ids = json.load(jfile)\n",
    "val_ids = [int(iid) for iid in val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weights = [1, 1, 0.2, 0.8, 0.7, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPatchesGenerator = PatchIterator(\"/home/lowik/sealion/data/sealion/\", train_ids, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valPatchesGenerator = PatchIterator(\"/home/lowik/sealion/data/sealion/\", val_ids, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_x, batch_counts in valPatchesGenerator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_pred = unet.predict(batch_x, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True counts:  [4 1 1 1 9]\n",
      "Pred counts:  [ 0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAFpCAYAAACBNaNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDLkkFWfIJak4Qy5JxRlySSrOkEtScf8Pmsnz4sSiBXQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8aac097eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 8\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(valPatchesGenerator.denormalize_input(batch_x[i,:,:, 0]))\n",
    "print(\"True counts: \", batch_counts[i, ...])\n",
    "print(\"Pred counts: \", counts_pred[i, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.15751839   5.15751839   4.62601328   5.03984165   5.15751839\n",
      "   5.09901857   0.44721362   4.62601328   4.47213602  17.88295174\n",
      "  17.88295174   5.38516474   0.99999994  17.01176071   1.09544516\n",
      "  12.336936  ]\n"
     ]
    }
   ],
   "source": [
    "y_true = K.constant(batch_counts)\n",
    "y_pred = K.constant(counts_pred)\n",
    "with sess.as_default():\n",
    "    print(rmse(y_true, y_pred).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /home/lowik/data/models/segmentation_debug_counts/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cb_checkpoint = ModelCheckpoint(\"/home/lowik/data/models/segmentation_debug_counts/ckpt_{epoch:02d}-{val_loss:.2f}.h5\")\n",
    "cb_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1, mode='auto', epsilon=0.01, cooldown=0, min_lr=0)\n",
    "tensorboard_cb = TensorBoardCallBack(log_dir=\"/home/lowik/data/models/segmentation_debug_counts/log_tb\", batch_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=1e-4, momentum=0.9, decay=1e-6, nesterov=True)\n",
    "adam = Adam()\n",
    "unet.compile(optimizer=adam, loss=mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 26s - loss: 150.9825 - val_loss: 116.5071\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 28s - loss: 175.8449 - val_loss: 90.6749\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 35s - loss: 152.8784 - val_loss: 90.7718\n",
      "Epoch 4/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 97.0972\n",
      "Epoch 00003: reducing learning rate to 0.00010000000474974513.\n",
      "250/250 [==============================] - 28s - loss: 97.0727 - val_loss: 99.0423\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 28s - loss: 213.6276 - val_loss: 67.3813\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 28s - loss: 94.2778 - val_loss: 98.7607\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 27s - loss: 137.6866 - val_loss: 60.5309\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 28s - loss: 177.6061 - val_loss: 64.4373\n",
      "Epoch 9/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 118.2185\n",
      "Epoch 00008: reducing learning rate to 1.0000000474974514e-05.\n",
      "250/250 [==============================] - 28s - loss: 118.1843 - val_loss: 66.3469\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 27s - loss: 166.5421 - val_loss: 55.5573\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 26s - loss: 107.2708 - val_loss: 80.1120\n",
      "Epoch 12/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 81.6531\n",
      "Epoch 00011: reducing learning rate to 1.0000000656873453e-06.\n",
      "250/250 [==============================] - 27s - loss: 81.4056 - val_loss: 90.1714\n",
      "Epoch 13/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 87.5800\n",
      "Epoch 00012: reducing learning rate to 1.0000001111620805e-07.\n",
      "250/250 [==============================] - 26s - loss: 87.8233 - val_loss: 60.1055\n",
      "Epoch 14/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 96.0530\n",
      "Epoch 00013: reducing learning rate to 1.000000082740371e-08.\n",
      "250/250 [==============================] - 27s - loss: 95.7271 - val_loss: 72.5744\n",
      "Epoch 15/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 95.7463\n",
      "Epoch 00014: reducing learning rate to 1.000000082740371e-09.\n",
      "250/250 [==============================] - 27s - loss: 98.4849 - val_loss: 70.5682\n",
      "Epoch 16/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 155.4448\n",
      "Epoch 00015: reducing learning rate to 1.000000082740371e-10.\n",
      "250/250 [==============================] - 27s - loss: 154.9009 - val_loss: 87.8186\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 28s - loss: 125.8833 - val_loss: 51.0998\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 28s - loss: 104.6574 - val_loss: 82.3401\n",
      "Epoch 19/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 106.8658\n",
      "Epoch 00018: reducing learning rate to 1.000000082740371e-11.\n",
      "250/250 [==============================] - 28s - loss: 113.9222 - val_loss: 84.6702\n",
      "Epoch 20/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 90.2992\n",
      "Epoch 00019: reducing learning rate to 1.000000082740371e-12.\n",
      "250/250 [==============================] - 28s - loss: 89.9963 - val_loss: 55.2351\n"
     ]
    }
   ],
   "source": [
    "h = unet.fit_generator(trainPatchesGenerator, 250, epochs=20,\n",
    "                       verbose=1, callbacks=[cb_checkpoint, cb_reduce_lr, tensorboard_cb],\n",
    "                       validation_data=valPatchesGenerator, validation_steps=100,\n",
    "                       class_weight=None,\n",
    "                       max_q_size=1, workers=1, pickle_safe=False,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch_x, counts in valPatchesGenerator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_pred = unet.predict(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = unet.get_layer(\"scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    print(l.weights[0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(valPatchesGenerator.denormalize_input(batch_x[i,:,:, 0]))\n",
    "print(\"True counts: \", batch_counts[i, ...])\n",
    "print(\"Pred counts: \", counts_pred[i, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unet.save(\"../data/unet_segmentation_ellipse_dmap_sgd_10epochs_200steps.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch_x, batch_y in valPatchesGenerator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_ypred = unet.predict(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(batch_ypred[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gg = np.argmax(batch_ypred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.unique(gg, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(valPatchesGenerator.denormalize_input(batch_x[i,:,:, :]))\n",
    "plt.subplot(132)\n",
    "plt.imshow(np.argmax(batch_y[i,...], axis=-1))\n",
    "plt.subplot(133)\n",
    "plt.imshow(np.argmax(batch_ypred[i,...], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(valPatchesGenerator.denormalize_output(batch_ypred[i,:,:, 0]) > 0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 7\n",
    "print(\"GT: \", np.sum(valPatchesGenerator.denormalize_output(batch_y[i,:,:, 0])))\n",
    "print(\"Pred: \", np.sum(valPatchesGenerator.denormalize_output(batch_ypred[i,:,:, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def full_image_process(im, net, patchGenerator, patch_size=(224, 224), batch_size=8, overlap=(64, 64)):\n",
    "    h, w, c = im.shape\n",
    "    n_patches_x = int(np.ceil((w - patch_size[1])/(patch_size[1] - overlap[1]) + 1))\n",
    "    n_patches_y = int(np.ceil((h - patch_size[0])/(patch_size[0] - overlap[0]) + 1))\n",
    "    print(n_patches_x, n_patches_x)\n",
    "    \n",
    "    dmap = np.zeros((h, w, 1), dtype=np.float32)\n",
    "    dmap_count = np.zeros((h, w, 1), dtype=np.int8)\n",
    "    batch_x = np.zeros((batch_size, ) + patch_size + (c, ), dtype=np.float32)\n",
    "    batch_pos = np.zeros((batch_size, 4), dtype=np.int32)\n",
    "    \n",
    "    current_batch_size = 0\n",
    "    for py in range(n_patches_y):\n",
    "        y_start = py * (patch_size[0] - overlap[0])\n",
    "        y_start = min(h - patch_size[0], y_start)\n",
    "        y_end = y_start + patch_size[0]\n",
    "        for px in range(n_patches_x):\n",
    "            x_start = px * (patch_size[1] - overlap[1])\n",
    "            x_start = min(w - patch_size[1], x_start)\n",
    "            x_end = x_start + patch_size[1]\n",
    "            \n",
    "            # Keep filling the batch\n",
    "            batch_x[current_batch_size, :, :, :] = im[y_start:y_end, x_start:x_end, :]\n",
    "            batch_pos[current_batch_size, :] = np.array([y_start, y_end, x_start, x_end])\n",
    "            current_batch_size += 1\n",
    "            \n",
    "            if current_batch_size == batch_size or (py == n_patches_y - 1 and px == n_patches_x - 1) :\n",
    "                # time to predict\n",
    "                batch_x_normed = patchGenerator.normalize_input(batch_x)\n",
    "                batch_ylog = net.predict(batch_x_normed)\n",
    "                batch_y = patchGenerator.denormalize_output(batch_ylog)\n",
    "                # Fill the full dmap\n",
    "                for i in range(current_batch_size):\n",
    "                    y_start, y_end, x_start, x_end = tuple(batch_pos[i,:])\n",
    "                    dmap[y_start:y_end, x_start:x_end, :] += batch_y[i,:,:,:]\n",
    "                    dmap_count[y_start:y_end, x_start:x_end] += 1\n",
    "                current_batch_size = 0\n",
    "                \n",
    "    return dmap, dmap_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = cv2.imread(\"../data/sealion/Train/872.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmap, dmap_count = full_image_process(im, unet, valPatchesGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(dmap[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmap_avg = dmap/dmap_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmap_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(dmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(dmap_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmap_gt = np.load(\"../data/sealion/TrainDensity/872_0.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
